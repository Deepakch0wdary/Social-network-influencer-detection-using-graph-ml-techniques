# -*- coding: utf-8 -*-
"""detecting influencer using ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DRjldeRy-o_c7UTeH2BWGczCQ0mkrOMX
"""

#!/usr/bin/env python3
"""
Influencer detection pipeline with FAKE-INFLUENCER detection added.

What this adds:
 - After computing features and running the RandomForest model, the script
   flags "fake influencers" among the model-predicted influencers using simple,
   practical heuristics (out/in follow ratio, low importance, low community embed).
 - Adds columns:
     - model_pred, model_prob  (existing)
     - fake_influencer (1 = flagged fake, 0 = not flagged)
     - fake_reason (short text explaining why it was flagged)
 - Prints the flagged fake influencers and reasons.
 - Highlights fake influencers in the network plot (black border + 'X' annotation).
 - Saves everything to influencer_features.csv and ranked_influencers.csv.

Heuristics (configurable in code):
 - out_in_ratio = out_degree / (in_degree + 1)
 - Condition A (bot-like): out_in_ratio > 5 AND pagerank < 25th percentile
 - Condition B (low-embed): clustering < 10th percentile AND core < 25th percentile
 - Condition C (many outgoing follows compared to incoming): out_degree > in_degree * 5 + 10
 - If any condition true for nodes with model_pred==1, mark as fake_influencer.

Run:
  python detect_influencers_from_csv_with_fake_detection.py

Requirements:
  pip install networkx pandas scikit-learn matplotlib numpy
"""
import os
import math
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, accuracy_score, roc_auc_score,
    roc_curve, confusion_matrix
)

# ---------------------------
# Config
# ---------------------------
EDGE_CSV = "edges.csv"
LABELS_CSV = "labels.csv"
OUT_DIR = "plots"
RANKED_OUT = "ranked_influencers.csv"
FEATURES_OUT = "influencer_features.csv"
RANDOM_STATE = 42
TEST_SIZE = 0.30

os.makedirs(OUT_DIR, exist_ok=True)

# ---------------------------
# 1) Load edges and labels
# ---------------------------
if not os.path.exists(EDGE_CSV):
    raise FileNotFoundError(f"{EDGE_CSV} not found in current directory. Create it and retry.")

edges = pd.read_csv(EDGE_CSV)
if not {'source','target'}.issubset(edges.columns):
    raise ValueError("edges.csv must contain columns: source,target")

labels = None
if os.path.exists(LABELS_CSV):
    labels = pd.read_csv(LABELS_CSV)
    if not {'node','label'}.issubset(labels.columns):
        raise ValueError("labels.csv must contain columns: node,label")
else:
    print("labels.csv not found — script will create heuristic labels if needed.")

# ---------------------------
# 2) Build directed graph (then convert to undirected for some measures if needed)
# ---------------------------
G_dir = nx.from_pandas_edgelist(edges, source='source', target='target', create_using=nx.DiGraph())
# Ensure labels nodes exist in graph
if labels is not None:
    for n in labels['node'].unique():
        if n not in G_dir:
            G_dir.add_node(n)

# We'll compute both directed and undirected measures
G = G_dir.to_undirected()

print(f"Graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges (undirected view)")

# ---------------------------
# 3) Compute structural features
# ---------------------------
nodes = list(G.nodes())

# Directed degrees
in_deg = dict(G_dir.in_degree())
out_deg = dict(G_dir.out_degree())
total_deg = {n: in_deg.get(n,0) + out_deg.get(n,0) for n in nodes}

# Undirected measures
pagerank = nx.pagerank(G, alpha=0.85)
try:
    eigen = nx.eigenvector_centrality_numpy(G)
except Exception:
    eigen = {n: 0.0 for n in nodes}

# Betweenness (approx if large)
n_nodes = G.number_of_nodes()
if n_nodes <= 300:
    betweenness = nx.betweenness_centrality(G, normalized=True)
else:
    k = min(200, max(10, int(0.1 * n_nodes)))
    betweenness = nx.betweenness_centrality(G, k=k, normalized=True, seed=RANDOM_STATE)

closeness = nx.closeness_centrality(G)
clustering = nx.clustering(G)
core = nx.core_number(G)

features = pd.DataFrame({
    'node': nodes,
    'in_degree': [in_deg.get(n,0) for n in nodes],
    'out_degree': [out_deg.get(n,0) for n in nodes],
    'total_degree': [total_deg.get(n,0) for n in nodes],
    'pagerank': [pagerank.get(n,0.0) for n in nodes],
    'eigenvector': [eigen.get(n,0.0) for n in nodes],
    'betweenness': [betweenness.get(n,0.0) for n in nodes],
    'closeness': [closeness.get(n,0.0) for n in nodes],
    'clustering': [clustering.get(n,0.0) for n in nodes],
    'core': [core.get(n,0) for n in nodes],
})

print("\nSample features:")
print(features.head())

# ---------------------------
# 4) Merge labels or create heuristic labels if missing
# ---------------------------
if labels is not None:
    features = features.merge(labels[['node','label']], on='node', how='left')
    # if any node missing label, fill with 0 (or you can choose another strategy)
    features['label'] = features['label'].fillna(0).astype(int)
else:
    # Heuristic label: top 3% by combined score
    pct = 0.03
    k = max(1, int(pct * len(features)))
    features['combined_score'] = (
        features['pagerank'].rank(method='average', pct=True) * 0.5 +
        features['total_degree'].rank(method='average', pct=True) * 0.3 +
        features['betweenness'].rank(method='average', pct=True) * 0.2
    )
    features = features.sort_values('combined_score', ascending=False).reset_index(drop=True)
    features['label'] = 0
    features.loc[:k-1, 'label'] = 1
    # reorder rows back to node alphabetical order for consistency
    features = features.sort_values('node').reset_index(drop=True)
    # drop combined_score column if present
    if 'combined_score' in features.columns:
        features.drop(columns=['combined_score'], inplace=True)

print("\nLabels distribution:")
print(features['label'].value_counts())

# ---------------------------
# 5) Prepare data for ML (Random Forest)
# ---------------------------
feature_cols = ['in_degree','out_degree','total_degree','pagerank','eigenvector','betweenness','closeness','clustering','core']
X = features[feature_cols].values
y = features['label'].values

# If only one class present, we cannot train — fallback to heuristic ranking
if len(np.unique(y)) <= 1:
    print("Only one class present in labels — skipping supervised training. Producing heuristic ranking.")
    features['score'] = (
        features['pagerank'].rank(method='average', pct=True) * 0.6 +
        features['total_degree'].rank(method='average', pct=True) * 0.4
    )
    ranked = features.sort_values('score', ascending=False).reset_index(drop=True)
    ranked[['node','score']].to_csv(RANKED_OUT, index=False)
    features.to_csv(FEATURES_OUT, index=False)
    print(f"Saved heuristic ranked influencers -> {RANKED_OUT}")
    print(f"Saved features -> {FEATURES_OUT}")
    # Visualize network with top-k highlighted
    topk = ranked.head(max(1, int(0.03*len(ranked))))['node'].tolist()
    pos = nx.spring_layout(G, seed=RANDOM_STATE)
    plt.figure(figsize=(8,8))
    colors = ['red' if n in topk else 'skyblue' for n in G.nodes()]
    sizes = [300 if n in topk else 100 for n in G.nodes()]
    nx.draw_networkx_edges(G, pos, alpha=0.3)
    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=sizes)
    nx.draw_networkx_labels(G, pos, font_size=9)
    plt.title("Heuristic influencers (red)")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, "network_vis_heuristic.png"), dpi=150)
    plt.show()
    raise SystemExit("Finished heuristic ranking (no supervised labels available).")

# train/test split (stratify by y)
X_train, X_test, y_train, y_test, nodes_train, nodes_test = train_test_split(
    X, y, features['node'].values, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

clf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, class_weight='balanced')
clf.fit(X_train_s, y_train)

y_pred = clf.predict(X_test_s)
y_proba = clf.predict_proba(X_test_s)[:,1] if hasattr(clf, "predict_proba") else None

print("\n=== Classification report (test set) ===")
print(classification_report(y_test, y_pred, digits=4, zero_division=0))
print("Accuracy:", accuracy_score(y_test, y_pred))
auc = None
if y_proba is not None and len(np.unique(y_test)) > 1:
    try:
        auc = roc_auc_score(y_test, y_proba)
        print("ROC AUC:", round(auc,4))
    except Exception:
        auc = None

# ---------------------------
# 6) Map predictions back to full features dataframe
# ---------------------------
features['pred_label'] = -1
features['pred_prob'] = np.nan
# fill predictions only for test nodes
for node, pred, prob in zip(nodes_test, y_pred, (y_proba if y_proba is not None else [None]*len(y_pred))):
    idx = features.index[features['node'] == node].tolist()
    if idx:
        i = idx[0]
        features.at[i, 'pred_label'] = int(pred)
        if prob is not None:
            features.at[i, 'pred_prob'] = float(prob)

# For train nodes, include predicted labels if desired (optional). We'll store model's prediction for all nodes:
all_probs = clf.predict_proba(scaler.transform(features[feature_cols].values))[:,1]
all_preds = clf.predict(scaler.transform(features[feature_cols].values))
features['model_pred'] = all_preds
features['model_prob'] = all_probs

# ---------------------------
# 7) FAKE-INFLUENCER DETECTION (heuristic rules)
# ---------------------------
# compute dynamic thresholds (percentiles) to make heuristics adaptive
pagerank_25 = features['pagerank'].quantile(0.25)
clustering_10 = features['clustering'].quantile(0.10)
core_25 = features['core'].quantile(0.25)

# prepare blank columns
features['fake_influencer'] = 0
features['fake_reason'] = ""

# helper function to evaluate and mark fake influencers
def mark_fake(row):
    in_d = row['in_degree']
    out_d = row['out_degree']
    pager = row['pagerank']
    clust = row['clustering']
    cr = row['core']
    out_in_ratio = out_d / (in_d + 1)  # avoid division by zero

    reasons = []
    # Condition A: bot-like following pattern (follows many, followed by few) & low importance
    if out_in_ratio > 5 and pager < pagerank_25:
        reasons.append(f"high_out_in_ratio({out_in_ratio:.1f})_low_pagerank")
    # Condition B: low community embed + low core (not embedded but predicted influencer)
    if clust < clustering_10 and cr < core_25:
        reasons.append("low_clustering_and_low_core")
    # Condition C: extreme outgoing follows absolute
    if out_d > in_d * 5 + 10:
        reasons.append("extreme_outgoing_follows")
    # Condition D: extremely low betweenness & closeness (no bridging or reach)
    if row['betweenness'] < 1e-4 and row['closeness'] < features['closeness'].quantile(0.25):
        reasons.append("low_betweenness_and_low_closeness")
    return out_in_ratio, reasons

# Only check nodes that model predicts as influencers (model_pred==1)
for idx, r in features.iterrows():
    if int(r['model_pred']) == 1:
        ratio, reasons = mark_fake(r)
        if reasons:
            features.at[idx, 'fake_influencer'] = 1
            features.at[idx, 'fake_reason'] = ";".join(reasons)

# Print flagged fake influencers
fake_list = features[features['fake_influencer'] == 1]
if not fake_list.empty:
    print("\n*** Flagged FAKE influencer candidates (model_pred==1 but suspicious): ***")
    for _, r in fake_list.iterrows():
        print(f"- {r['node']}: reason = {r['fake_reason']}, out/in_ratio = {r['out_degree']/(r['in_degree']+1):.2f}, pagerank={r['pagerank']:.6f}, clustering={r['clustering']:.4f}, core={int(r['core'])}")
else:
    print("\nNo fake influencer candidates flagged.")

# ---------------------------
# 8) Save ranked + features CSV (includes fake flags & reasons)
# ---------------------------
ranked = features.sort_values('model_prob', ascending=False).reset_index(drop=True)
ranked[['node','model_prob','model_pred','fake_influencer','fake_reason']].to_csv(RANKED_OUT, index=False)
features.to_csv(FEATURES_OUT, index=False)
print(f"\nSaved ranked influencers -> {RANKED_OUT}")
print(f"Saved features + predictions -> {FEATURES_OUT}")

# ---------------------------
# 9) Plots (centrality, ROC, confusion, network)
# ---------------------------
def plot_top_centralities(df, top_n=8, savepath=os.path.join(OUT_DIR,"centrality_bar.png")):
    top_pr = df.sort_values('pagerank', ascending=False).head(top_n)
    top_deg = df.sort_values('total_degree', ascending=False).head(top_n)
    fig, axs = plt.subplots(1,2, figsize=(14,6))
    axs[0].barh(top_pr['node'].astype(str)[::-1], top_pr['pagerank'][::-1])
    axs[0].set_title(f"Top {top_n} by PageRank")
    axs[0].set_xlabel("PageRank")
    axs[1].barh(top_deg['node'].astype(str)[::-1], top_deg['total_degree'][::-1])
    axs[1].set_title(f"Top {top_n} by Total Degree")
    axs[1].set_xlabel("Total Degree")
    plt.tight_layout()
    plt.savefig(savepath, dpi=150)
    plt.show()
    print(f"Saved centrality bar plot -> {savepath}")

def plot_roc(y_true, y_score, savepath=os.path.join(OUT_DIR,"roc_curve.png")):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    plt.figure(figsize=(6,5))
    plt.plot(fpr, tpr, linewidth=2)
    plt.plot([0,1],[0,1],'--', linewidth=1)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(savepath, dpi=150)
    plt.show()
    print(f"Saved ROC curve -> {savepath}")

def plot_confusion(y_true, y_pred, savepath=os.path.join(OUT_DIR,"confusion_matrix.png")):
    cm = confusion_matrix(y_true, y_pred)
    labels_names = ['non-influencer','influencer']
    fig, ax = plt.subplots(figsize=(4,4))
    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    ax.figure.colorbar(im, ax=ax)
    ax.set_xticks(np.arange(len(labels_names))); ax.set_yticks(np.arange(len(labels_names)))
    ax.set_xticklabels(labels_names); ax.set_yticklabels(labels_names)
    plt.xlabel('Predicted'); plt.ylabel('True')
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.savefig(savepath, dpi=150)
    plt.show()
    print(f"Saved confusion matrix -> {savepath}")

def plot_network(G_obj, features_df, savepath=os.path.join(OUT_DIR,"network_vis.png")):
    pos = nx.spring_layout(G_obj, seed=RANDOM_STATE)
    plt.figure(figsize=(10,10))
    # Draw edges first
    nx.draw_networkx_edges(G_obj, pos, alpha=0.3)
    # Draw nodes with colors & sizes depending on true/pred/fake status
    for n in G_obj.nodes():
        row = features_df[features_df['node'] == n]
        if row.empty:
            color = 'lightgray'
            size = 80
            edgecolor = 'k'
        else:
            r = row.iloc[0]
            true = int(r['label'])
            pred = int(r['model_pred'])
            fake = int(r['fake_influencer'])
            # color scheme:
            # darkred = TP influencer, red = FN, orange = FP, blue = TN
            if true == 1 and pred == 1:
                color = 'darkred'
                size = 400
            elif true == 1 and pred == 0:
                color = 'red'
                size = 350
            elif true == 0 and pred == 1:
                color = 'orange'
                size = 300
            else:
                color = 'skyblue'
                size = 150
            # if flagged fake, draw black border and annotate with 'X'
            edgecolor = 'black' if fake == 1 else 'k'
        nx.draw_networkx_nodes(G_obj, pos, nodelist=[n], node_color=color, node_size=size, edgecolors=edgecolor, linewidths=2)
    # labels
    nx.draw_networkx_labels(G_obj, pos, font_size=9)
    # annotate fake influencers with a small 'X' text near node
    for _, r in features_df[features_df['fake_influencer'] == 1].iterrows():
        xy = pos[r['node']]
        plt.text(xy[0]+0.02, xy[1]+0.02, "X", fontsize=12, fontweight='bold', color='black')
    plt.title("Network: darkred=TP influencer, red=FN, orange=FP, blue=TN. Black edge/X = flagged fake influencer")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(savepath, dpi=150)
    plt.show()
    print(f"Saved network visualization -> {savepath}")

# Generate plots
plot_top_centralities(features, top_n=min(8, len(features)))
if auc is not None:
    plot_roc(y_test, y_proba)
plot_confusion(y_test, y_pred)
plot_network(G, features)

# ---------------------------
# 10) Explanation block (project-specific metric meanings)
# ---------------------------
print("\n\n==================== GRAPH METRIC MEANINGS (PROJECT CONTEXT) ====================\n")

print("1. Betweenness Centrality (Information Bridge Score):")
print("   → Shows how often a user lies on the shortest communication paths between other users.")
print("   → In influencer detection, users with HIGH betweenness act as BRIDGES connecting different groups.")
print("   → They are important because information must pass THROUGH them, so they can control or speed up spread.\n")

print("2. Closeness Centrality (Reachability / Spread Speed Score):")
print("   → Measures how close a user is to all other users in the network.")
print("   → Users with HIGH closeness can reach every other user with FEWER steps.")
print("   → This means they can spread information FASTER, making them strong influencers.\n")

print("3. Clustering Coefficient (Community Influence Score):")
print("   → Shows how strongly a user's friends are connected to each other.")
print("   → HIGH clustering = user is in a tightly connected community (great for local influence).")
print("   → LOW clustering = user connects different communities (great for global influence).")
print("   → Both types can be influencers depending on the situation.\n")

print("4. Core Number (K-core Influence Depth):")
print("   → Shows how deeply a user is embedded inside the network.")
print("   → Users with HIGH core value belong to the DENSE, INNER PART of the network.")
print("   → These users often have stronger and more stable influence because they are part of a tightly connected core.\n")

print("===============================================================================\n")

# ---------------------------
# 11) Final prints
# ---------------------------
print("\nTop 10 ranked influencers (by model probability):")
print(ranked[['node','model_prob','model_pred','fake_influencer']].head(10).to_string(index=False))

print("\nTrue influencers (label=1):")
print(features.loc[features['label']==1, 'node'].tolist())

print("\nFlagged fake influencers (if any):")
if not fake_list.empty:
    print(fake_list[['node','model_prob','model_pred','fake_influencer','fake_reason']].to_string(index=False))
else:
    print("None")

print("\nScript completed. Outputs:")
print(f" - {RANKED_OUT}")
print(f" - {FEATURES_OUT}")
print(f" - plots in {OUT_DIR}/")